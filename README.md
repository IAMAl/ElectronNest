# Modern Processors and Their Issues

## Challenges in Modern Processors and Their Issues

Modern processors use pipelined architecture to increase instruction throughput by dividing the execution of instructions into several stages. However, to achieve maximum performance, the pipeline stages need to be fully filled, and empty stages can be caused by various factors, such as the misprediction of dynamic control-flow caused by conditional branch instructions and the delay caused by waiting for data to arrive from external memory. To mitigate these issues, modern architectures can be limited by control-flow delays and data dependency issues. To address these concerns, processors have introduced techniques such as branch prediction, cache memory, and out-of-order execution.

### Challenges

The advantage of branch prediction is that it can significantly reduce the delay caused by conditional branch instructions, resulting in higher instruction throughput and improved performance. However, branch prediction is not always accurate, and incorrect predictions can lead to wasted processing cycles and decreased performance. Additionally, predicting the correct branch path requires analysis of the program's control-flow, which can be a complex and computationally expensive task. As a result, improving the accuracy of branch prediction is an ongoing challenge for modern processor designers. Cache memory reduces the impact of loading data from external memory, but cache misses can lead to delays and empty pipeline stages. In addition, finding independent instructions is a challenging problem, and it is done by selecting candidate instructions in an instruction window, which is a pool of fetched instructions. This task requires data dependency analysis on a chip, which can limit the scope of the instruction window. Out-of-order execution is another technique used by modern processors to maintain performance and overcome the impact of control-flow delays caused by branch misprediction. This technique allows the processor to execute instructions out-of-order that maximize the utilization of pipeline stages and computational resources, even if the instructions are not in the original program order. By analyzing the dependencies among instructions and rearranging their execution, the processor can fully fill pipeline stages and maintain a higher level of instruction throughput.

### Issues

While these techniques have improved performance, they also present ongoing challenges in terms of accuracy, scalability, and power efficiency. As technology markets continue to evolve at a rapid pace, modern processors have become increasingly important for meeting the demands of high-performance and power efficient computing. However, even with advanced pipelined architectures, processors can still face issues such as branch misprediction and delays caused by loading data from external memory. In today's rapidly evolving technology markets, there is a growing demand for high-performance and power efficient computing systems that can meet the needs of a wide range of applications. Given the trends and these challenges, there is a need for innovative solutions that can balance the performance, flexibility, and cost considerations in designing computing systems. In this context, the coarse-grained reconfigurable array (CGRA) architecture and compiler have emerged as promising alternatives to traditional CPU and GPU-based systems. By offering customizable and efficient hardware and software components, CGRAs can be tailored to meet specific application requirements and provide a flexible platform for future adaptation. CGRA architectures and compilers allow developers to customize and optimize both the hardware and software components of a computing system, providing a high degree of performance and efficiency for specific application domains. However, developing CGRA-based systems can be challenging and expensive, requiring specialized hardware and software expertise, as well as complex design and manufacturing processes.

## Our Proposals: General-Purpose CGRA

Our proposal aims to address these challenges by exploring new approaches to developing a general-purpose CGRA architecture and compiler that can simplify the design and implementation process, while also providing the flexibility and adaptability required to meet the changing requirements of the market. The goal of this proposal is to create an architecture and compiler that are well-suited for a wide range of applications and can be easily customized and adapted to meet specific requirements. Using RISC philosophy for both the architecture and compiler can simplify the design and implementation of both components and can help ensure that they are well-suited for a wide range of applications. Using an existing common compiler frontend can simplify the development process; since it leverages existing infrastructure and tools to generate the necessary IR code for the CGRA architecture. The CGRA architecture can offer advantages in terms of power efficiency and design flexibility, which can be important for certain types of applications.

### Benefit over ASICs and DSLs

Compared to ASICs and DSL-based accelerators, developing a general-purpose CGRA architecture and compiler can potentially offer lower non-recurring engineering (NRE) and manufacturing costs, since they can be used for multiple applications and do not require specialized design and manufacturing processes. This can make the CGRA architecture and compiler more accessible to a wider range of applications and developers. The adaptability and flexibility of a general-purpose CGRA architecture and compiler can help ensure that the system remains relevant and valuable over the long term, even as the requirements and technology of the market change. This can help reduce the risk of being locked into a specific hardware or software technology that may become obsolete over time and can help to ensure that the investment in CGRA-based systems remains valuable and relevant over the long term.

### Steering Control-Flow

Our approach to solving the steering control-flow is to prepare a path for a condition signal to select a source and or a destination. This approach could help to reduce the impact of control-flow on the pipeline by ensuring that instructions are executed in the correct order. By dynamically selecting the appropriate source and/or destination based on the condition signal, it could avoid pipeline stalls and maintain the original algorithm, even in the presence of complex control-flow. By using distributed memories on a chip, which is located close to the processing elements, the latency of data transfers can be reduced, which can help to minimize the impact of data dependencies and control-flow on the performance. The on-chip memory can reduce the latency and area cost associated with the memory hierarchy, and it can simplify the design of the system, by eliminating the need for a separate memory hierarchy. This could reduce the area cost of the system, and also reduce the complexity of the memory access architecture, making it easier to manage. While on-chip memory is expensive, by applying many chips of CGRA, the cost can be reduced. Using many chips of CGRA can also provide other benefits, such as scalability and flexibility. By using multiple chips, it may be possible to scale the system to handle larger workloads or to add or remove chips as needed to optimize the system for specific workloads. Additionally, by using a distributed architecture, the system can be designed to be more fault-tolerant, as failures in one chip can be isolated and handled without affecting the rest of the system.

### Use of Data Dependency

The elements on a chip are connected with a two-dimensional mesh, and the chips are also arranged in a two-dimensional array which can be extended to connect the datapaths on different chips. This approach can simplify the design of the system, by using a regular array structure for the chips and the mesh connections. Additionally, by using data dependency-based configuration to connect the datapaths, the system can be designed to be highly configurable and optimized for specific workloads. The chaining-based execution approach with data dependency, could be used to connect datapaths on different chips. This approach can simplify the communication and synchronization between the different chips, as the chaining approach ensures that the data is transferred in the correct order and that there are no pipeline stalls. One potential issue with this approach is that it may not offer the same level of performance and optimization for specific application domains as ASICs and DSL-based accelerators. This is because ASICs and DSL-based accelerators are specifically designed for the targeted application domains, which allows them to achieve high levels of performance and optimization. However, by carefully considering the design and implementation process and optimizing the hardware and software for the specific application domain, general-purpose CGRA and its compiler may be able to overcome this issue and provide competitive performance and optimization with ASICs and DSL-based accelerators, while still offering the benefits of a more accessible and adaptable platform.



# Proposal Architecture

### Take Data Dependencies, NOT Take Independent Operations

As mentioned in the previous section, steering the data dependency has a key role in microarchitecture. We aggressively use data dependency to configure datapath rather than finding independent operations. Our research focuses on the nature of data-level parallelism obtained from the spatial configuration like a graph configuration which can not be obtained in the case of traditional microprocessor-based computers. Key components are a processing element, a memory element, and an interconnection network. Two elements are replicated on a chip with a two-dimensional arrangement of a checkered pattern. The regular pattern makes scaling the array possible, and simplifies and supports a process of compilation, especially for placement and routing. The processing element performs common operations such as arithmetic and logic operations. The memory element stores data that may be temporally used and copied for distribution to relax communication to many destinations.

### Basic Ideas

Both types of elements consist of the same interconnection network components like a template that standardizes the microarchitecture. By embedding a user's module into the template, a user-specific element can be implemented. In addition, the template-based interconnection helps to design and verify it by the precise verification points. Our interconnection network consists of two types of simple routers. One router is called a FanOut link element that transfers data and supports unicast. Another router is called a FanIn link element that selects one source from candidates coming from the FanOut link elements. These two types of link elements are connected, FanOut link element is connected to FanIn link elements and vice versa. We do not take a hierarchical interconnection network that introduces complexity and thus higher workload for placement and routing, difficulty to scale the array as less compatibility for variants.

Any information in our microarchitecture has the same rules and format to run. Any information is treated as a message having its length. The message consists of one or more blocks that consist of the same type or attribution of routing data, configuration data for processing and memory elements, and data. The block has a limited length. The first word of the block is an information word about the block called an attribute word. The attribute word has some roles such as the type indication, length of the block, and so on. The message flows on the array with a worm-whole routing. Therefore, the proposed microarchitecture is a kind of message-passing. The message is stored in the element, and loaded from the element. We call these actions, a push and a pull, respectively. At first, a message is pushed and pulled the message to another place. Our computation model belongs only to these two types of actions.

There is no central controller. In order to keep an order of processing, a message has an Identification (My-ID) and two other IDs called true-ID and false-ID. FanIn link element checks the My-ID of a message and selects one when My-ID is matched with one of the true-ID and false-ID that are stored by a previous message. One of the true-ID and false-ID is selected by a condition signal making in-order use of the link element and a conditional routing. The condition signal used for conditional routing comes from the processing or memory element. In order to remove a controller for pipeline stall, we also propose a new pipeline register that avoids dropping data by stalling. We use a handshake protocol of request (req) and not-acknowledge (nack). A common protocol needs double cycles to transfer thus making half throughput. Our pipeline register architecture keeps a single data word transfer per cycle. This approach removes an effort to place a path for source operands that need the same timing to feed into ALU, thus common architecture needs unnecessary paths and routings. Thus our architecture reduces the workload for scheduling, placement and routing at compilation, and thus data-flow graph can be configured on the array without taking care of the timing.